# batch_size_exp
Deep dive into LLM inference batch sizes with PyTorch and vLLM - exploring throughput scaling, latency tradeoffs, GPU profiling, and CUDA kernel analysis through hands-on experiments
